good paper: https://proceedings.mlr.press/v54/mcmahan17a/mcmahan17a.pdf
this is the fedavg initial paper
just for me to save the link


also save this paper:
https://arxiv.org/pdf/2301.09604
it nicely explains server step instead of simple aggregation if fedavg


fed hyper paper: adjusment of global learning rates (on server and clients) and local learning rates on clients:
https://openreview.net/pdf?id=Kl9CqKf7h6

his paper seems to discuss the effect of data distribtuions on the weights:
https://arxiv.org/pdf/1806.00582


specaugment, gaussian noise, speed pertubation
https://arxiv.org/html/2303.00510v2
