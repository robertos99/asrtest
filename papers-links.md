good paper: https://proceedings.mlr.press/v54/mcmahan17a/mcmahan17a.pdf
this is the fedavg initial paper
just for me to save the link


also save this paper:
https://arxiv.org/pdf/2301.09604
it nicely explains server step instead of simple aggregation if fedavg


fed hyper paper: adjusment of global learning rates (on server and clients) and local learning rates on clients:
https://openreview.net/pdf?id=Kl9CqKf7h6

his paper seems to discuss the effect of data distribtuions on the weights:
https://arxiv.org/pdf/1806.00582


specaugment, gaussian noise, speed pertubation
https://arxiv.org/html/2303.00510v2


defines noniid iid
https://arxiv.org/html/2401.00809v1#:~:text=Non%2DIID%20data%20implies%20that,same%20underlying%20distribution%20breaks%20down.

https://www.isca-archive.org/interspeech_2015/ko15_interspeech.pdf
speed pertubation asr
